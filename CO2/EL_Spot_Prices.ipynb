{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1eJX-mE1Akcj80jl8DVYDeP_K3EFb0Xja",
      "authorship_tag": "ABX9TyP6PrP6ESgr1yQGyBK68iZg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tallerzalan/Energinet/blob/main/CO2/EL_Spot_Prices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "XOwb6Ok4ViqK",
        "outputId": "a99a43f1-121d-458d-a510-b54353b79201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 46 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 49.5 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=4bdffe358733d1ebb16284ee8188169f03e5d3719aead83c198a9402c419f734\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f25fb64e0d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://59988e0a026e:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Install pyspark\n",
        "!pip install pyspark\n",
        "\n",
        "# Import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark Session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# Check Spark Session Information\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "import pyspark.sql.types as T\n",
        "\n",
        "from pyspark.sql.functions import col, to_timestamp"
      ],
      "metadata": {
        "id": "17Sp_3q1ayMz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_date_columns(df, timestamp_column = 'timestamp'):\n",
        "    \"\"\"\n",
        "    Create a year, month and day column from the given timestamp column.\n",
        "    Month and day column values will be zero-padded, i.e.: '01', '02', '03' etc.\n",
        "    \"\"\"\n",
        "    # Remove columns created by spark data loading process, i.e.: 'day', 'month', 'year'\n",
        "    df = df.drop('day', 'month', 'year')\n",
        "    \n",
        "    df = df.withColumn('year', F.year(timestamp_column))\\\n",
        "           .withColumn('month', F.date_format(timestamp_column, 'MM'))\\\n",
        "           .withColumn('day', F.date_format(timestamp_column, 'dd'))\n",
        "    \n",
        "    return df"
      ],
      "metadata": {
        "id": "w5HmPGy4ayl5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Price areas in Denmark\n",
        "price_area = ['DK1', 'DK2']\n",
        "\n",
        "# Municipality numbers in DK1\n",
        "dk1_list = ['751', '851', '461', '630', '561', '730', '791', '740', '621', '615', '657', '540', '860', '746', '813', '661', '479', '580', '760', '510', '430', '607', '573', '710', '766', '779', '706', '787', '575', '846',\n",
        "            '756', '420', '410', '849', '550', '707', '820', '810', '450', '840', '480', '530', '440', '727', '671', '773', '665', '482', '492', '741', '563', '825']\n",
        "\n",
        "# Municipality numbers in DK2\n",
        "dk2_list = ['101', '147', '265', '370', '330', '157', '316', '159', '217', '259', '376', '230', '173', '167', '169', '219', '253', '151', '326', '250', '390', '240', '185', '175', '190', '210', '270', '360', '400', '320',\n",
        "            '153', '329', '306', '260', '340', '163', '350', '165', '201', '223', '269', '183', '336', '161', '187', '155']"
      ],
      "metadata": {
        "id": "QLUOzQCFa-Na"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the EL Spot Prices data\n",
        "df = spark.read.json('/content/drive/MyDrive/Colab Notebooks/TDK/Spot Price/Elspotprices.json')\\\n",
        "               .dropDuplicates()\\\n",
        "               .drop('HourDK', 'SpotPriceEUR')\n",
        "\n",
        "# Cleaning and manipulating the EL Spot Prices data\n",
        "df = df\\\n",
        "     .withColumn('converted', F.to_timestamp('HourUTC'))\\\n",
        "     .drop('HourUTC')\n",
        "\n",
        "dk_1 = df\\\n",
        "       .filter(col('PriceArea') == 'DK1')\\\n",
        "       .groupBy(F.date_trunc('hour', F.col('converted')).alias('date'))\\\n",
        "       .agg(F.sum('SpotPriceDKK').alias('value'))\n",
        "\n",
        "dk_2 = df\\\n",
        "       .filter(col('PriceArea') == 'DK2')\\\n",
        "       .groupBy(F.date_trunc('hour', F.col('converted')).alias('date'))\\\n",
        "       .agg(F.sum('SpotPriceDKK').alias('value'))\n",
        "\n",
        "dk_1.sort('date').show(truncate = False)\n",
        "dk_2.sort('date').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLj_BhnqbBCr",
        "outputId": "61dbaebf-1d4f-44da-8c9a-aba7e506b340"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----------+\n",
            "|date               |value      |\n",
            "+-------------------+-----------+\n",
            "|2022-09-30 22:00:00|478.950012 |\n",
            "|2022-09-30 23:00:00|472.850006 |\n",
            "|2022-10-01 00:00:00|371.799988 |\n",
            "|2022-10-01 01:00:00|159.720001 |\n",
            "|2022-10-01 02:00:00|125.739998 |\n",
            "|2022-10-01 03:00:00|111.760002 |\n",
            "|2022-10-01 04:00:00|111.690002 |\n",
            "|2022-10-01 05:00:00|177.270004 |\n",
            "|2022-10-01 06:00:00|334.769989 |\n",
            "|2022-10-01 07:00:00|543.200012 |\n",
            "|2022-10-01 08:00:00|816.840027 |\n",
            "|2022-10-01 09:00:00|793.940002 |\n",
            "|2022-10-01 10:00:00|669.23999  |\n",
            "|2022-10-01 11:00:00|550.559998 |\n",
            "|2022-10-01 12:00:00|491.959991 |\n",
            "|2022-10-01 13:00:00|497.470001 |\n",
            "|2022-10-01 14:00:00|519.179993 |\n",
            "|2022-10-01 15:00:00|1024.670044|\n",
            "|2022-10-01 16:00:00|1261.660034|\n",
            "|2022-10-01 17:00:00|1455.73999 |\n",
            "+-------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-------------------+-----------+\n",
            "|date               |value      |\n",
            "+-------------------+-----------+\n",
            "|2022-09-30 22:00:00|478.950012 |\n",
            "|2022-09-30 23:00:00|472.850006 |\n",
            "|2022-10-01 00:00:00|371.799988 |\n",
            "|2022-10-01 01:00:00|159.720001 |\n",
            "|2022-10-01 02:00:00|125.739998 |\n",
            "|2022-10-01 03:00:00|111.760002 |\n",
            "|2022-10-01 04:00:00|111.690002 |\n",
            "|2022-10-01 05:00:00|177.270004 |\n",
            "|2022-10-01 06:00:00|334.769989 |\n",
            "|2022-10-01 07:00:00|543.200012 |\n",
            "|2022-10-01 08:00:00|816.840027 |\n",
            "|2022-10-01 09:00:00|793.940002 |\n",
            "|2022-10-01 10:00:00|669.23999  |\n",
            "|2022-10-01 11:00:00|550.559998 |\n",
            "|2022-10-01 12:00:00|491.959991 |\n",
            "|2022-10-01 13:00:00|497.470001 |\n",
            "|2022-10-01 14:00:00|519.179993 |\n",
            "|2022-10-01 15:00:00|1024.670044|\n",
            "|2022-10-01 16:00:00|1259.060059|\n",
            "|2022-10-01 17:00:00|1455.73999 |\n",
            "+-------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VRPoFd4tbQG6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}