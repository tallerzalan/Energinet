{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1RRFEM50VJdKk8abeb_nKkz6-jHFPHzSs",
      "authorship_tag": "ABX9TyPCqMe3GSL7u3tfza8UhScT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tallerzalan/Energinet/blob/main/CO2/CO2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "aBeRzO5uNJNk",
        "outputId": "0fdf371b-f0a0-4030-c249-7a4529069424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 47 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 41.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=2c6dca5c308f29b3ffd2c4f80226028d51dea558f48a10e4bd27cd233974934a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fae16439790>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://4c6741411c1c:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Install pyspark\n",
        "!pip install pyspark\n",
        "\n",
        "# Import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark Session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# Check Spark Session Information\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import subprocess\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from datetime import datetime, timedelta, date\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "import pyspark.sql.types as T\n",
        "\n",
        "from pyspark.sql.functions import col, to_timestamp"
      ],
      "metadata": {
        "id": "AfVkLPJJNeGz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_date_columns(df, timestamp_column = 'timestamp'):\n",
        "    \"\"\"\n",
        "    Create a year, month and day column from the given timestamp column.\n",
        "    Month and day column values will be zero-padded, i.e.: '01', '02', '03' etc.\n",
        "    \"\"\"\n",
        "    # Remove columns created by spark data loading process, i.e.: 'day', 'month', 'year'\n",
        "    df = df.drop('day', 'month', 'year')\n",
        "    \n",
        "    df = df.withColumn('year', F.year(timestamp_column))\\\n",
        "           .withColumn('month', F.date_format(timestamp_column, 'MM'))\\\n",
        "           .withColumn('day', F.date_format(timestamp_column, 'dd'))\n",
        "    \n",
        "    return df"
      ],
      "metadata": {
        "id": "9b_7J1oCOj-D"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Municipality numbers in DK1\n",
        "dk1_list = ['751', '851', '461', '630', '561', '730', '791', '740', '621', '615', '657', '540', '860', '746', '813', '661', '479', '580', '760', '510', '430', '607', '573', '710', '766', '779', '706', '787', '575', '846',\n",
        "            '756', '420', '410', '849', '550', '707', '820', '810', '450', '840', '480', '530', '440', '727', '671', '773', '665', '482', '492', '741', '563', '825']\n",
        "\n",
        "# Municipality numbers in DK2\n",
        "dk2_list = ['101', '147', '265', '370', '330', '157', '316', '159', '217', '259', '376', '230', '173', '167', '169', '219', '253', '151', '326', '250', '390', '240', '185', '175', '190', '210', '270', '360', '400', '320',\n",
        "            '153', '329', '306', '260', '340', '163', '350', '165', '201', '223', '269', '183', '336', '161', '187', '155']"
      ],
      "metadata": {
        "id": "roBCqLv_Omih"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the CO2 data\n",
        "df = spark.read.json('/content/drive/MyDrive/Colab Notebooks/Energinet/CO2/CO2Emis.json')\\\n",
        "               .dropDuplicates()\\\n",
        "               .drop('Minutes5DK')\n",
        "\n",
        "# Cleaning and manipulating the CO2 data\n",
        "df = df\\\n",
        "     .withColumn('converted', F.to_timestamp('Minutes5UTC'))\\\n",
        "     .drop('Minutes5UTC')\n",
        "\n",
        "dk_1 = df\\\n",
        "       .filter(col('PriceArea') == 'DK1')\\\n",
        "       .groupBy(F.date_trunc('hour', F.col('converted')).alias('date'))\\\n",
        "       .agg(F.mean('CO2Emission').alias('value'))\n",
        "\n",
        "dk_2 = df\\\n",
        "       .filter(col('PriceArea') == 'DK2')\\\n",
        "       .groupBy(F.date_trunc('hour', F.col('converted')).alias('date'))\\\n",
        "       .agg(F.mean('CO2Emission').alias('value'))\n",
        "\n",
        "dk_1_clean = add_date_columns(dk_1, timestamp_column = 'date')\\\n",
        "             .withColumn('timestamp', F.date_format('date', \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\\\n",
        "             .withColumn('category', F.lit('co2_per_production'))\\\n",
        "             .drop('date')\n",
        "\n",
        "dk_2_clean = add_date_columns(dk_2, timestamp_column = 'date')\\\n",
        "             .withColumn('timestamp', F.date_format('date', \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\\\n",
        "             .withColumn('category', F.lit('co2_per_production'))\\\n",
        "             .drop('date')\n",
        "\n",
        "dk_1_clean.sort('timestamp').show(truncate = False)\n",
        "dk_2_clean.sort('timestamp').show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJo25SW1PJ69",
        "outputId": "15d96f3a-b037-41b7-9a2c-09b04aff8f8a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+----+-----+---+--------------------+------------------+\n",
            "|value             |year|month|day|timestamp           |category          |\n",
            "+------------------+----+-----+---+--------------------+------------------+\n",
            "|47.916666666666664|2022|09   |30 |2022-09-30T22:00:00Z|co2_per_production|\n",
            "|49.083333333333336|2022|09   |30 |2022-09-30T23:00:00Z|co2_per_production|\n",
            "|35.083333333333336|2022|10   |01 |2022-10-01T00:00:00Z|co2_per_production|\n",
            "|35.083333333333336|2022|10   |01 |2022-10-01T01:00:00Z|co2_per_production|\n",
            "|38.5              |2022|10   |01 |2022-10-01T02:00:00Z|co2_per_production|\n",
            "|39.083333333333336|2022|10   |01 |2022-10-01T03:00:00Z|co2_per_production|\n",
            "|38.25             |2022|10   |01 |2022-10-01T04:00:00Z|co2_per_production|\n",
            "|36.0              |2022|10   |01 |2022-10-01T05:00:00Z|co2_per_production|\n",
            "|38.583333333333336|2022|10   |01 |2022-10-01T06:00:00Z|co2_per_production|\n",
            "|54.083333333333336|2022|10   |01 |2022-10-01T07:00:00Z|co2_per_production|\n",
            "|58.916666666666664|2022|10   |01 |2022-10-01T08:00:00Z|co2_per_production|\n",
            "|75.41666666666667 |2022|10   |01 |2022-10-01T09:00:00Z|co2_per_production|\n",
            "|114.16666666666667|2022|10   |01 |2022-10-01T10:00:00Z|co2_per_production|\n",
            "|142.33333333333334|2022|10   |01 |2022-10-01T11:00:00Z|co2_per_production|\n",
            "|149.66666666666666|2022|10   |01 |2022-10-01T12:00:00Z|co2_per_production|\n",
            "|124.33333333333333|2022|10   |01 |2022-10-01T13:00:00Z|co2_per_production|\n",
            "|140.0             |2022|10   |01 |2022-10-01T14:00:00Z|co2_per_production|\n",
            "|146.0             |2022|10   |01 |2022-10-01T15:00:00Z|co2_per_production|\n",
            "|146.25            |2022|10   |01 |2022-10-01T16:00:00Z|co2_per_production|\n",
            "|120.25            |2022|10   |01 |2022-10-01T17:00:00Z|co2_per_production|\n",
            "+------------------+----+-----+---+--------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+------------------+----+-----+---+--------------------+------------------+\n",
            "|value             |year|month|day|timestamp           |category          |\n",
            "+------------------+----+-----+---+--------------------+------------------+\n",
            "|47.916666666666664|2022|09   |30 |2022-09-30T22:00:00Z|co2_per_production|\n",
            "|49.083333333333336|2022|09   |30 |2022-09-30T23:00:00Z|co2_per_production|\n",
            "|35.083333333333336|2022|10   |01 |2022-10-01T00:00:00Z|co2_per_production|\n",
            "|35.083333333333336|2022|10   |01 |2022-10-01T01:00:00Z|co2_per_production|\n",
            "|38.5              |2022|10   |01 |2022-10-01T02:00:00Z|co2_per_production|\n",
            "|39.083333333333336|2022|10   |01 |2022-10-01T03:00:00Z|co2_per_production|\n",
            "|38.25             |2022|10   |01 |2022-10-01T04:00:00Z|co2_per_production|\n",
            "|36.0              |2022|10   |01 |2022-10-01T05:00:00Z|co2_per_production|\n",
            "|38.583333333333336|2022|10   |01 |2022-10-01T06:00:00Z|co2_per_production|\n",
            "|54.083333333333336|2022|10   |01 |2022-10-01T07:00:00Z|co2_per_production|\n",
            "|58.916666666666664|2022|10   |01 |2022-10-01T08:00:00Z|co2_per_production|\n",
            "|75.41666666666667 |2022|10   |01 |2022-10-01T09:00:00Z|co2_per_production|\n",
            "|114.16666666666667|2022|10   |01 |2022-10-01T10:00:00Z|co2_per_production|\n",
            "|142.33333333333334|2022|10   |01 |2022-10-01T11:00:00Z|co2_per_production|\n",
            "|149.66666666666666|2022|10   |01 |2022-10-01T12:00:00Z|co2_per_production|\n",
            "|124.33333333333333|2022|10   |01 |2022-10-01T13:00:00Z|co2_per_production|\n",
            "|140.0             |2022|10   |01 |2022-10-01T14:00:00Z|co2_per_production|\n",
            "|146.0             |2022|10   |01 |2022-10-01T15:00:00Z|co2_per_production|\n",
            "|146.25            |2022|10   |01 |2022-10-01T16:00:00Z|co2_per_production|\n",
            "|120.25            |2022|10   |01 |2022-10-01T17:00:00Z|co2_per_production|\n",
            "+------------------+----+-----+---+--------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gQhP8enpSY9R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}